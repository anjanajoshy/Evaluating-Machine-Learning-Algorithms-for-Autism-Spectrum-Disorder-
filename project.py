# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1skTX_ZW0q0O3VSfJZnTu2cJ8RjKX5LmT
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import precision_recall_curve, classification_report
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV,  cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression



# Ignoring the warnings
import warnings
warnings.filterwarnings("ignore")

"""**DATASET**"""

data = pd.read_csv('/content/data_csv.csv')
data.head()

data.tail()

data.shape

data.info()

# Identifying numerical columns
numerical_cols = data.select_dtypes(include=['int64', 'float64', 'uint8']).columns
print("Numerical columns:", numerical_cols)

# Identifying categorical columns
categorical_cols = data.select_dtypes(include=['object', 'category', 'bool']).columns
print("Categorical columns:", categorical_cols)

#counting values of each features
for column in data.columns:
    print(f"Value counts for {column}:")
    print(data[column].value_counts())
    print("\n")

#sorting age
age =data.sort_values(by = 'Age_Years')
age['Age_Years']

data.describe()

"""**Exploratory Data Analysis**"""

#plotting ASD_traits
traits_count = data['ASD_traits'].value_counts().sort_index()

# Creating a new DataFrame for clear labeling
traits_df = pd.DataFrame({
    'Trait': ['Positive', 'Negative'],
    'Counts': [traits_count['Yes'], traits_count['No']]
})

# Plotting the bar chart
plt.figure(figsize=(10, 6))
plt.bar(traits_df['Trait'], traits_df['Counts'], color='purple')

# Setting the title and labels with increased font sizes
plt.title('ASD Result', fontsize=20)
plt.xlabel('Autism Test Result', fontsize=18)
plt.ylabel('Counts', fontsize=18)

# Setting the font size of the ticks on both axes
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)

# Save and show the plot
plt.savefig('ASD_Traits_Result.png')
plt.show()

#bar chart for Gender
sns.countplot(x='ASD_traits', hue='Sex', data=data)
plt.title('Autism count by Gender')
plt.xlabel('Autism Test Result', fontsize=18)
plt.ylabel('Counts', fontsize=18)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.savefig("Bar chart for Gender")
plt.show()

# Plot the histogram for the age column
plt.hist(data['Age_Years'], bins=range(data['Age_Years'].min(), data['Age_Years'].max() + 1), color='blue', edgecolor='black')

# Add titles and labels
plt.title('Age Distribution of Autism Diagnosis', fontsize = 18)
plt.xlabel('Age', fontsize = 16)
plt.ylabel('Frequency', fontsize = 16)

# Show all age values on the x-axis
plt.xticks(range(data['Age_Years'].min(), data['Age_Years'].max() + 1), fontsize= 14)
plt.savefig("histogram for Age")
plt.show()

#selecting columns for pie chart
selected_columns = data[['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10_Autism_Spectrum_Quotient']]

# Determine the layout of the subplots
num_rows = 3
num_cols = 4
plt.figure(figsize=(30,26))

# Set font size for labels and autopct
label_fontsize = 25
autopct_fontsize = 25

# Loop through the columns and create a pie chart in each subplot
for i, column in enumerate(selected_columns):
    ax = plt.subplot(num_rows, num_cols, i+1)
    value_counts = selected_columns[column].value_counts()
    ax.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=90,
           textprops={'fontsize': label_fontsize})
    plt.title(column, fontsize=label_fontsize)

plt.subplots_adjust(top=0.93)
plt.suptitle("Pie Chart Distribution of Autism Spectrum Quotient Variables", fontsize=30)
plt.savefig('A1 to A10 Quotient variables')
plt.show()

"""**Methodology**

Feature Renaming
"""

#Normalize 'Ethnicity' and 'Who_completed_the_test' columns to consistent case
data['Ethnicity'] = data['Ethnicity'].str.lower()
data['Who_completed_the_test'] = data['Who_completed_the_test'].str.lower()

# Map variations of ethnicity names to a standardized name
ethnicity_mapping = {
    'asian': 'asian',
    'south asian': 'south asian',
    'middle eastern': 'middle eastern',
    'black': 'black',
    'mixed': 'mixed'
}
data['Ethnicity'] = data['Ethnicity'].map(ethnicity_mapping).fillna(data['Ethnicity'])

who_mapping = {
    'family member': 'family member'
}
data['Who_completed_the_test'] = data['Who_completed_the_test'].map(who_mapping).fillna(data['Who_completed_the_test'])

#value counts to see the consolidated categories
print("Value counts for Ethnicity:")
print(data['Ethnicity'].value_counts())
print("\nValue counts for Who_completed_the_test:")
print(data['Who_completed_the_test'].value_counts())

# Mapping of long column names to shorter ones
short_column_names = {
    "A10_Autism_Spectrum_Quotient": "A10 Quotient",
    "Social_Responsiveness_Scale": "Responsiveness scale",
    "Global developmental delay/intellectual disability": "GDD/Intellect Disability",
    "Social/Behavioural Issues": "Social/Behavior Issues",
    "Speech Delay/Language Disorder": "Language Disorder",
    "Childhood Autism Rating Scale": "Autism rating",
    "Family_mem_with_ASD": "Family_ASD",
    "Who_completed_the_test": "Test_Completed_By"
}

# Renaming the columns
data.rename(columns=short_column_names, inplace=True)

ethnicity_counts = data['Ethnicity'].value_counts()

# Create a dot plot
fig, ax = plt.subplots(figsize=(10, 8))
ethnicities = ethnicity_counts.index[::-1]
counts = ethnicity_counts.values[::-1]

# Define a base color
colors = plt.cm.get_cmap('Set3', len(ethnicities))

# Plotting each row of dots in reversed order
for i, (ethnicity, count) in enumerate(zip(ethnicities, counts)):
    # Generate a color for each ethnicity
    color = colors(i)
    # Create an array with the size of the count for the current ethnicity
    y = np.full(count, i)
    # Plotting
    ax.scatter(range(count), y, color=color, edgecolor='black', s=100, label=ethnicity, alpha=0.6)

# Setting the y-ticks
ax.set_yticks(range(len(ethnicities)))
ax.set_yticklabels(ethnicities, fontsize = 18)
ax.legend(title='Ethnicity', fontsize = 18)

ax.set_title('Dot Plot of Ethnicity Distribution', fontsize = 18)
plt.savefig('dot_plot_ethnicity.png')
plt.show()

# Calculate percentages from the counts
test_counts_percentage = data['Test_Completed_By'].value_counts(normalize=True) * 100

# Create a horizontal bar chart with percentages
plt.figure(figsize=(10, 6))
test_counts_percentage.plot(kind='barh', color='teal',fontsize = 14)
plt.title('Who Completed the Test (Percentage)', fontsize = 14)
plt.xlabel('Percentage',fontsize = 14)
plt.ylabel('Category', fontsize = 14)
plt.savefig("Who completed the test")
plt.show()

"""**Data Cleaning**"""

# Creating the DataFrame with missing values count
missing_values_df = pd.DataFrame(data.isnull().sum(), columns=["With Missing Values"])

# Saving the DataFrame to a CSV file
missing_values_df.to_csv("with missing values.csv", index=True)
missing_values_df

#handling missing values
selected_columns = ['Responsiveness scale','Qchat_10_Score']
for column in selected_columns:
    if column in data.columns:
        data[column].fillna(data[column].mean(), inplace=True)
data['Social/Behavior Issues'].fillna(data['Social/Behavior Issues'].mode()[0], inplace=True)
data['Depression'].fillna(data['Depression'].mode()[0], inplace=True)

# Creating the DataFrame with missing values count
missing_values_df = pd.DataFrame(data.isnull().sum(), columns=["Without Missing Values"])

# Saving the DataFrame to a CSV file
missing_values_df.to_csv("without missing values.csv", index=True)
missing_values_df

#dropping CASE_NO_PATIENT'S
data = data.drop(columns =["CASE_NO_PATIENT'S"])
data.info()

# Count the number of duplicate rows
number_of_duplicates = data.duplicated().sum()

# Print out the number of duplicate rows found
print(f"Number of duplicate rows before dropping duplicates: {number_of_duplicates}")

# Drop the duplicate rows
data = data.drop_duplicates()

# Count the number of duplicate rows to verify that duplicates have been removed
number_of_duplicates_after = data.duplicated().sum()

# Print out the number of duplicate rows after dropping duplicates
print(f"Number of duplicate rows after dropping duplicates: {number_of_duplicates_after}")

#encoding to change categorical variable to numerical variable
binary_columns = ['Language Disorder', 'Learning disorder', 'Genetic_Disorders', 'Depression', 'GDD/Intellect Disability', 'Social/Behavior Issues', 'Anxiety_disorder','Family_ASD', 'Jaundice','ASD_traits']
for column in binary_columns:
    data[column] = data[column].map({'Yes': 1, 'No': 0})
data

#plotting heatmap to find the feature correlation
plt.figure(figsize=(20, 20))
heatmap = sns.heatmap(data.corr(numeric_only=True), annot=True, cmap='coolwarm',fmt='.2f',
                      vmin = -1, vmax = 1, annot_kws={'size': 14}, linewidths=1, linecolor='black')
plt.title('Heatmap of Variable Correlations', fontsize = '42')

heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90, horizontalalignment='right', fontsize='23')
heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0, fontsize='23')
plt.savefig("Heatmap")
plt.show()

# One-hot encode nominal columns
data = pd.get_dummies(data, columns=['Sex', 'Ethnicity', 'Test_Completed_By'],drop_first=False)
data

df_original = data.copy()
#Function to identify outliers using IQR method
def identify_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return outliers

# Function to handle outliers using IQR method
def handle_outliers(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    data[column] = data[column].clip(lower=lower_bound, upper=upper_bound)
    return data

# Identify outliers in numerical columns
numeric_cols = data.select_dtypes(include='number').columns
columns_with_outliers_before = {}
for col in numeric_cols:
    outliers = identify_outliers(data, col)
    if not outliers.empty:
        columns_with_outliers_before[col] = len(outliers)

# Handle outliers
for col in numeric_cols:
    data = handle_outliers(data, col)

# Check for outliers after handling
columns_with_outliers_after = {}
for col in numeric_cols:
    outliers_after = identify_outliers(data, col)
    if not outliers_after.empty:
        columns_with_outliers_after[col] = len(outliers_after)

print("Columns with outliers before handling and their counts:")
for col, count in columns_with_outliers_before.items():
    print(f"{col}: {count}")

print("\nColumns with outliers after handling and their counts:")
for col, count in columns_with_outliers_after.items():
    print(f"{col}: {count}")

# Create a DataFrame from the 'columns_with_outliers_before' dictionary
outlier_comparison = pd.DataFrame(list(columns_with_outliers_before.items()), columns=['Column', 'Outliers Before Handling'])

# Add a new column to this DataFrame for the 'columns_with_outliers_after' data
outlier_comparison['Outliers After Handling'] = outlier_comparison['Column'].map(columns_with_outliers_after)
outlier_comparison['Outliers After Handling'].fillna(0, inplace=True)
outlier_comparison['Outliers After Handling'] = outlier_comparison['Outliers After Handling'].astype(int)

# Display the pairwise comparison table with a bar representation
outlier_comparison.style.bar(subset=['Outliers Before Handling', 'Outliers After Handling'])
outlier_comparison.to_csv("outlier_comparison.csv", index=False)
outlier_comparison

#setting target and features
Y = data['ASD_traits']
X = data.drop(columns = ['ASD_traits'])

#Separate out a test set
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Initialize the scaler
scaler = StandardScaler()

# Fit the scaler on the training set only
scaler.fit(X_train)

# Transform the training, and test sets
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply PCA
pca = PCA(n_components=0.95)
pca.fit(X_train_scaled)

# Transform the scaled datasets
X_train_pca = pca.transform(X_train_scaled)
X_test_pca = pca.transform(X_test_scaled)

"""**SUPPORT VECTOR MACHINE**"""

# Initialize SVC with linear kernel
svc = SVC(kernel='linear', C=0.1, random_state=42)

# Apply RFE for feature selection
selector = RFE(estimator=svc, n_features_to_select=15, step=1)
X_train_selected = selector.fit_transform(X_train_pca, y_train)
X_test_selected = selector.transform(X_test_pca)

# Train SVC on selected features
svc.fit(X_train_selected, y_train)

# Predictions and evaluation
y_train_pred = svc.predict(X_train_selected)
y_test_pred = svc.predict(X_test_selected)

#print accuracy
print("Training Accuracy:", accuracy_score(y_train, y_train_pred))
print("Test Accuracy:", accuracy_score(y_test, y_test_pred))
print("\nClassification Report for Test Set:\n", classification_report(y_test, y_test_pred))

cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', cbar=False, annot_kws={"size": 16})
plt.title('Confusion Matrix for the Test Set', fontsize = 16)
plt.xlabel('Predicted Label', fontsize = 16)
plt.ylabel('True Label', fontsize = 16)
plt.savefig("initial confusion matrix of svm")
plt.show()

selected_coefficients = svc.coef_[0]

# generate component names
selected_features = ['Component ' + str(i+1) for i in range(len(selected_coefficients))]

# Plotting the feature importance
plt.figure(figsize=(10, 8))
plt.barh(selected_features, selected_coefficients, color='blue')
plt.xlabel('Coefficient Value')
plt.ylabel('PCA Component')
plt.title('Selected PCA Components for SVM')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest coefficient at the top
plt.savefig("SVM feature selection")
plt.show()

# Get the PCA components
loadings = pca.components_
feature_names = np.array(X.columns)

# Choose the component
component_index = 0

# Set a threshold for filtering significant loadings only
threshold = 0.2

# Filter features based on the threshold
significant_loadings = loadings[component_index, :]
significant_features = feature_names[np.abs(significant_loadings) >= threshold] #selecting whose absolute values are equal to or exceed the threshold

# Plot the loadings for the significant features of the selected principal component
plt.figure(figsize=(10, 8))
plt.barh(significant_features, significant_loadings[np.abs(significant_loadings) >= threshold], color='blue')
plt.xlabel('Loading Value')
plt.ylabel('Original Feature')
plt.title(f'Feature Contributions to PCA Component 1 SVM')
plt.savefig("Features of component 1 svm")
plt.show()

# Define the parameter grid
param_grid = {
    'C': [0.01, 0.1,1],
    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Type of SVM kernel
    'gamma': ['scale', 'auto'],  # setting decision boundary for kernel
}

# Proceed with GridSearchCV using the selected features
grid_search = GridSearchCV(estimator=SVC(random_state=42), param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)
grid_search.fit(X_train_selected, y_train)

# Display the best parameters
print("Best parameters found: ", grid_search.best_params_)

# Use the best estimator to make predictions
best_model = grid_search.best_estimator_

y_train_pred_best = best_model.predict(X_train_selected)
y_test_pred_best = best_model.predict(X_test_selected)

# Calculate accuracies with the best model
train_accuracy_best = accuracy_score(y_train, y_train_pred_best)
test_accuracy_best = accuracy_score(y_test, y_test_pred_best)

# Generate and print the classification report for the test set
print("Classification Report (Test Set):")
print(classification_report(y_test, y_test_pred_best))

# Print accuracies after Grid Search
print(f"Training Accuracy: {train_accuracy_best}")
print(f"Test Accuracy: {test_accuracy_best}")

cm_test = confusion_matrix(y_test, y_test_pred_best)

# Visualize the confusion matrix using a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(cm_test, annot=True, fmt="d", cmap='Blues', cbar=False,
            annot_kws={"size": 20})  # Adjusting size for better readability

plt.title('Confusion Matrix of SVM', size=20)
plt.xlabel('Predicted Label', size=24)
plt.ylabel('True Label', size=24)
plt.xticks(size=20)
plt.yticks(size=20, rotation=0)
plt.savefig("Confusion matrix of final svm")
plt.show()

cv_scores = cross_val_score(best_model, X_train_selected, y_train, cv=5, scoring='accuracy')

# Calculate the mean and standard deviation of the cross-validation scores
cv_mean = np.mean(cv_scores)
cv_std = np.std(cv_scores)

# Print the results
print(f"Cross-Validation Accuracy: {cv_mean:.4f} (+/- {cv_std:.4f})")

train_scores = best_model.decision_function(X_train_selected)
test_scores = best_model.decision_function(X_test_selected)

# Calculate precision and recall for both sets
precision_train, recall_train, thresholds_train = precision_recall_curve(y_train, train_scores)
precision_test, recall_test, thresholds_test = precision_recall_curve(y_test, test_scores)

# plot the precision-recall curves for both
plt.figure(figsize=(10, 6))

# Plot training set curve
plt.plot(recall_train, precision_train, color='green', lw=2,  marker='.', label='Train Precision-Recall curve')

# Plot test set curve
plt.plot(recall_test, precision_test, color='blue', lw=2, marker='.', label='Test Precision-Recall curve')


plt.xlabel('Recall', fontsize=14)
plt.ylabel('Precision', fontsize=14)
plt.title('Precision-Recall Curve for SVM', fontsize=16)
plt.legend(loc="lower left", fontsize=12)
plt.savefig("Precision_recall_of_svm.png", dpi=300, bbox_inches='tight')
plt.show()

"""**Random** **Forest**"""

random_forest = RandomForestClassifier(n_estimators=100, random_state=42)
random_forest.fit(X_train_pca, y_train)

# Calculate and print accuracy using predictions on the training set
y_train_pred = random_forest.predict(X_train_pca)
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"Training Accuracy: {train_accuracy:.4f}")

# Calculate and print accuracy using predictions on the test set
y_test_pred = random_forest.predict(X_test_pca)
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Accuracy: {test_accuracy:.4f}")

# Print the classification report for the test set predictions
print("\nClassification Report for Test Set:")
print(classification_report(y_test, y_test_pred))

# Confusion Matrix for initial model
cm_initial = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(7, 5))
sns.heatmap(cm_initial, annot=True, fmt='d', cmap='Greens',cbar = False, xticklabels=True, yticklabels=True, annot_kws={"size": 12})
plt.title('Confusion Matrix - Initial Model', fontsize = 16)
plt.xlabel('Predicted Labels', fontsize = 16)
plt.ylabel('True Labels', fontsize = 16)
plt.savefig("Confusion matrix of initial RF")
plt.show()

importances = random_forest.feature_importances_
sorted_indices = importances.argsort()[::-1][:15]  # Get indices of top 15 features

# Generate component names
component_names = ['Component ' + str(i+1) for i in range(X_train_pca.shape[1])]

plt.figure(figsize=(10, 8))
plt.title('Top 15 Feature Importances Initial Random Forest Model')
plt.barh(range(15), importances[sorted_indices], align='center', color = 'green')
plt.yticks(range(15), [component_names[i] for i in sorted_indices])
plt.xlabel('Relative Importance')
plt.gca().invert_yaxis()
plt.savefig("Top component of RF")
plt.show()

# Get the loadings
loadings = pca.components_

component_number = 0  # for the first principal component

# Set a threshold for selecting significant features based on their loading scores
threshold = 0.02

# Get the loading scores for this component
loading_scores = pd.Series(loadings[component_number], index=X.columns)

# Select features that meet or exceed the threshold in absolute value
significant_features = loading_scores[loading_scores.abs() >= threshold]

# Sorting the significant features by their loading score magnitude
sorted_significant_features = significant_features.abs().sort_values(ascending=False)

# Plotting
plt.figure(figsize=(10, 8))
sorted_significant_features.plot(kind='barh', color='lightgreen')
plt.title(f' Features Contribution to Principal Component {component_number + 1} (Threshold: {threshold})')
plt.xlabel('Loading Score')
plt.tick_params(axis='y', labelsize=12)
plt.ylabel('Features')
plt.gca().invert_yaxis()  # Invert y-axis to have the highest value at the top
plt.savefig("Feature of component 1 of RF")
plt.show()

X_train_top15 = X_train_pca[:, sorted_indices]
X_test_top15 = X_test_pca[:, sorted_indices]

#Hyperparameter tuning with only the top 15 features
param_grid = {
    'n_estimators': [50, 75, 100],  # Reduced estimators to control complexity
    'max_depth': [5,10, 15, 20],  # Limited tree depth to avoid overfitting
    'min_samples_split': [5, 10, 15, 20],  # Increased minimum samples to split
    'min_samples_leaf': [10,15],  # Added minimum samples at leaf to regularize
    'max_features': ['sqrt', 'log2'],  # Control the number of features considered at each split
}

grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42),
                           param_grid=param_grid, cv=10, scoring='accuracy', n_jobs=-1, verbose=2)
grid_search.fit(X_train_top15, y_train)

print("Best parameters found:", grid_search.best_params_)

best_rf = RandomForestClassifier(**grid_search.best_params_, random_state=42)
best_rf.fit(X_train_top15, y_train)
# Predict with top 15 features on the training set
y_train_pred_final = best_rf.predict(X_train_top15)
# Predict with top 15 features
y_pred_final = best_rf.predict(X_test_top15)

# Calculate accuracy and print the classification report
accuracy_final = accuracy_score(y_test, y_pred_final)
accuracy_train_final = accuracy_score(y_train, y_train_pred_final)
print(f"Final Training Set Accuracy: {accuracy_train_final:.4f}")
print(f"Final Test Set Accuracy: {accuracy_final:.4f}")
print(classification_report(y_test, y_pred_final))

# Confusion Matrix for the final model
cm_final = confusion_matrix(y_test, y_pred_final)
plt.figure(figsize=(7, 5))
sns.heatmap(cm_final, annot=True, fmt='d', cmap='Greens',cbar = False, xticklabels=True, yticklabels=True, annot_kws={"size": 16})
plt.title('Confusion Matrix - Final Model', fontsize = 16)
plt.xlabel('Predicted Labels', fontsize = 16)
plt.ylabel('True Labels', fontsize = 16)
plt.savefig("Confusion matrix for RF")
plt.show()

# Perform 5-fold cross-validation and obtain the cross-validation scores
cv_scores = cross_val_score(random_forest, X_train_pca, y_train, cv=5, scoring='accuracy')

# Calculate the mean and standard deviation of the cross-validation scores
cv_scores_mean = cv_scores.mean()
cv_scores_std = cv_scores.std()

print(f"CV Accuracy: {cv_scores_mean:.4f} (+/- {cv_scores_std * 2:.4f})")
cv_scores_std

y_scores_train_best = best_rf.predict_proba(X_train_top15)[:, 1]  # Probabilities for the positive class on the training set
y_scores_test_best = best_rf.predict_proba(X_test_top15)[:, 1]  # Probabilities for the positive class on the test set

# Step 2: Calculate precision and recall for both sets
precision_train, recall_train, _ = precision_recall_curve(y_train, y_scores_train_best)
precision_test, recall_test, _ = precision_recall_curve(y_test, y_scores_test_best)

# Step 3: Plotting the curves
plt.figure(figsize=(8, 6))
plt.plot(recall_train, precision_train, marker='.', color='red', label='Train Precision-Recall Curve')
plt.plot(recall_test, precision_test, marker='.', color='green', label='Test Precision-Recall Curve')
plt.xlabel('Recall', fontsize=14)
plt.ylabel('Precision', fontsize=14)
plt.title('Precision-Recall Curve for RF', fontsize=16)
plt.legend(loc="lower left", fontsize=12)
plt.savefig("Precision_recall_of_RF.png", dpi=300, bbox_inches='tight')
plt.show()

"""**LOGISTIC REGRESSION**"""

# Initialize the Logistic Regression model for RFE
log_reg_for_rfe = LogisticRegression(random_state=42, max_iter=10000)

# Apply RFE for feature selection, aiming to select top 15 features
rfe = RFE(estimator=log_reg_for_rfe, n_features_to_select=15, step=1)
rfe.fit(X_train_pca, y_train)

# Select the features from training and test set based on RFE selection
X_train_selected = X_train_pca[:, rfe.support_]
X_test_selected = X_test_pca[:, rfe.support_]

# Train a new Logistic Regression model on the RFE-selected features
log_reg_selected = LogisticRegression(random_state=42, max_iter=10000)
log_reg_selected.fit(X_train_selected, y_train)

# Predict on the training set with the new model
y_train_pred = log_reg_selected.predict(X_train_selected)

# Predict on the test set with the new model
y_test_pred = log_reg_selected.predict(X_test_selected)

# Calculate and print the training accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
print(f"Training Set Accuracy: {train_accuracy:.4f}")

# Calculate and print the test accuracy
test_accuracy = accuracy_score(y_test, y_test_pred)
print(f"Test Set Accuracy: {test_accuracy:.4f}")

# Print the classification report for the test set predictions
print("\nClassification Report for Test Set:")
print(classification_report(y_test, y_test_pred))

conf_matrix = confusion_matrix(y_test, y_test_pred)

# Plot the confusion matrix
plt.figure(figsize=(6, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Reds", cbar=False, annot_kws={"size": 16})
plt.xlabel('Predicted Label', fontsize = 16)
plt.ylabel('True Label', fontsize = 16)
plt.title('Confusion Matrix for Logistic Regression - Initial Model')
plt.savefig("Inital Confusion metrices of lr")
plt.show()

coef = log_reg_selected.coef_[0]

# Create a range for the x-axis
feature_range = np.arange(len(coef)) + 1

# Plotting
plt.figure(figsize=(8, 10))
plt.barh(feature_range, coef, color='red')
plt.ylabel('Selected PCA Components')
plt.xlabel('Coefficient Value')
plt.title('Importance of Selected PCA Components')
plt.yticks(feature_range)
plt.gca().invert_yaxis()
plt.savefig("Top features of LR")
plt.show()

loadings = pca.components_[0]  # Loadings for the first component

# Threshold for filtering significant loadings only
threshold = 0.2

# Filter features and their loadings based on the threshold
significant_loadingslr = loadings[np.abs(loadings) > threshold]
significant_featureslr = np.array(feature_names)[np.abs(loadings) > threshold]

# Creating a range for the y-axis for the filtered features
feature_range = np.arange(len(significant_loadingslr)) + 1

# Plotting
plt.figure(figsize=(8, 10))
plt.barh(feature_range, significant_loadingslr, color='Red')
plt.ylabel('Features')
plt.xlabel('Loading Value')
plt.title('Feature Contributions to component 1 (threshold:0.02)')
plt.yticks(feature_range, significant_featureslr)
plt.gca().invert_yaxis()  # Invert y-axis
plt.savefig(" Feature os component 1 LR")
plt.show()

param_grid = {
    'C': np.logspace(-8, 8, 100),
    'solver': ['liblinear', 'lbfgs'],
        'max_iter': [500, 1000,5000, 10000]
}
grid_search = GridSearchCV(estimator=LogisticRegression(random_state=42), param_grid=param_grid, cv=15, scoring='accuracy', n_jobs=-1, verbose=2)
grid_search.fit(X_train_selected, y_train)
print("Best parameters found:", grid_search.best_params_)

# Train new Logistic Regression model with best parameters on RFE-selected features
best_log_reg = LogisticRegression(**grid_search.best_params_, random_state=42)
best_log_reg.fit(X_train_selected, y_train)

# Predict on the training set with the best model
y_train_pred_best = best_log_reg.predict(X_train_selected)

# Predict on the test set with the best model
y_test_pred_best = best_log_reg.predict(X_test_selected)
# Calculate and print the final training accuracy
final_train_accuracy = accuracy_score(y_train, y_train_pred_best)
print(f"Final Training Set Accuracy: {final_train_accuracy:.4f}")

# Calculate and print the final test accuracy
final_test_accuracy = accuracy_score(y_test, y_test_pred_best)
print(f"Final Test Set Accuracy: {final_test_accuracy:.4f}")

# Print the classification report for the test set predictions
print("\nFinal Classification Report for Test Set:")
print(classification_report(y_test, y_test_pred_best))

# Confusion Matrix for the final model
cm_final = confusion_matrix(y_test, y_test_pred_best)
plt.figure(figsize=(7, 5))
sns.heatmap(cm_final, annot=True, fmt='d', cmap='Reds',cbar = False,  xticklabels=True,
            yticklabels=True)
plt.title('Confusion Matrix - Final Logistic Regression Model')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.savefig("Confusion matrix LR")
plt.show()

# Perform cross-validation with 10-fold cross-validation
cv_scores = cross_val_score(best_log_reg, X_train_selected, y_train, cv=10, scoring='accuracy', n_jobs=-1)

# Calculate the mean and standard deviation of the cross-validation scores
cv_mean = cv_scores.mean()
cv_std = cv_scores.std()

print(f"Cross-Validation Accuracy: {cv_mean:.4f} (+/- {cv_std * 2:.4f})")

y_scores_lr = best_log_reg.decision_function(X_test_selected)
precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)

y_scores_train = best_log_reg.decision_function(X_train_selected)
# Calculate precision and recall for the training set
precision_train, recall_train, thresholds_train = precision_recall_curve(y_train, y_scores_train)

# Plotting both Training and Test Precision-Recall Curves
plt.figure(figsize=(10, 8))

# Plot for the test set
plt.plot(recall, precision, marker='.', color='maroon', label='Test Set - Logistic Regression')

# Plot for the training set
plt.plot(recall_train, precision_train, marker='.', color='purple', label='Training Set - Logistic Regression')

plt.xlabel('Recall', fontsize=14)
plt.ylabel('Precision', fontsize=14)
plt.title('Precision-Recall Curve for LR', fontsize=16)
plt.legend(loc="lower left", fontsize=12)
plt.savefig("Precision_recall_of_LR.png", dpi=300, bbox_inches='tight')
plt.show()